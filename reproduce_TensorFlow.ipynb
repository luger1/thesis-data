{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "REP_TensorFlow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "ZqXc2GhVDSHm"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "m_cKBgdiDSEk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TensorFlow with word embeddings to classify the [Ten Thousand German News Articles Dataset](https://github.com/tblock/10kGNAD)\n",
        "This Notebook contains the code to reproduce the results in my thesis.\n",
        "\n",
        "Run all cells consecutively.\n",
        "\n",
        "Since the code is based on the [TF-Hub text classification tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub), the original copyright notice is included.\n"
      ]
    },
    {
      "metadata": {
        "id": "ZqXc2GhVDSHm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "metadata": {
        "id": "4PCZXSvMEhVW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DP2SntkYEqbP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Enviroment Setup "
      ]
    },
    {
      "metadata": {
        "id": "y7vqrBwPEi9R",
        "colab_type": "code",
        "outputId": "2fe5a783-80df-4186-8c5a-8953607ac10a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "# load the dataset and generate subsets\n",
        "!rm -rf 10kGNAD lowshot\n",
        "!git config --global advice.detachedHead false\n",
        "!git clone -q --branch v1.1 https://github.com/tblock/10kGNAD.git && echo \"downloaded dataset\"\n",
        "!mkdir lowshot\n",
        "!cp 10kGNAD/train.csv .\n",
        "!python 10kGNAD/code/generate_lowshot_sets.py > /dev/null && echo \"generated train subsets\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloaded dataset\n",
            "generated train subsets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yoe4LMF3DI8F",
        "colab_type": "code",
        "outputId": "3765e169-9d3b-4b11-a014-af76c94e3053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install --quiet tensorflow>=1.7 tensorflow-hub"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.11.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.6.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.14.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (40.8.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pUDzgxFYFUWW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "os.environ[\"TFHUB_CACHE_DIR\"] = '/tmp/tfhub'\n",
        "tf.logging.set_verbosity(tf.logging.ERROR) # Reduce logging output\n",
        "tf.set_random_seed(42) \n",
        "np.random.seed(42) # set seeds for reproducabel results\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kfW7KAOvG4Wx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train Models"
      ]
    },
    {
      "metadata": {
        "id": "h22zFd2DGAPa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class_names = ['Etat', 'Inland', 'International', 'Kultur', 'Panorama', 'Sport', 'Web', 'Wirtschaft', 'Wissenschaft']\n",
        "def label_to_int(label_list):\n",
        "  label_list_as_int = []\n",
        "  for entry in label_list:\n",
        "    label_list_as_int.append(class_names.index(entry))\n",
        "  return label_list_as_int"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jlfGiBSeGms0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load test set and create input function\n",
        "test_df = pd.read_csv('10kGNAD/test.csv', header=None, sep=';', quotechar=\"'\", names=['label', 'text'])   # columns names if no header\n",
        "test_df = test_df.assign(label_int = label_to_int(test_df['label']))\n",
        "predict_test_input_fn = tf.estimator.inputs.pandas_input_fn(test_df, test_df[\"label_int\"], shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YZU3GqP_GpTK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_with_module(hub_module, train_input_fn, train_module=True):\n",
        "  \n",
        "    embedded_text_feature_column = hub.text_embedding_column(key=\"text\", module_spec=hub_module, trainable=train_module)\n",
        "\n",
        "    estimator = tf.estimator.DNNClassifier(\n",
        "        hidden_units=[500, 100],\n",
        "        feature_columns=[embedded_text_feature_column],\n",
        "        n_classes=9,\n",
        "        optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.035,\n",
        "                                                    l1_regularization_strength=0.001,\n",
        "                                                    l2_regularization_strength=0.001)\n",
        "    )\n",
        "    \n",
        "    estimator.train(input_fn=train_input_fn, steps=1000)\n",
        "    test_eval_result = estimator.evaluate(input_fn=predict_test_input_fn)\n",
        "    test_set_accuracy = test_eval_result[\"accuracy\"]\n",
        "    \n",
        "    return test_set_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3mjtcrE4Grxz",
        "colab_type": "code",
        "outputId": "7f30d4bb-b9fa-48fd-fa36-28f776ad1ed3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1637
        }
      },
      "cell_type": "code",
      "source": [
        "filenames = sorted(glob.glob(\"lowshot/*.csv\"))\n",
        "\n",
        "for filename in filenames:\n",
        "  \n",
        "    # load testset and create input function\n",
        "    train_df = pd.read_csv(filename, header=None, sep=';', quotechar=\"'\", names=['label', 'text'])   # columns names if no header\n",
        "    train_df = train_df.assign(label_int = label_to_int(train_df['label']))\n",
        "    train_input_fn = tf.estimator.inputs.pandas_input_fn(train_df, train_df[\"label_int\"], num_epochs=None, shuffle=True)\n",
        "    \n",
        "    acc = train_and_evaluate_with_module(\"https://tfhub.dev/google/nnlm-de-dim128-with-normalization/1\", train_input_fn, True)\n",
        "    \n",
        "    print(filename,\"%.2f\" % float((100 - acc*100)), sep=\"\\t\")\n",
        "    \n",
        "    !rm -rf /tmp/tmp* # remove all tmp/tmp* folders since the old trained modells fill up the disk space"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lowshot/lowshot_0_0.01_0.csv\t33.17\n",
            "lowshot/lowshot_0_0.01_1.csv\t28.40\n",
            "lowshot/lowshot_0_0.01_2.csv\t33.07\n",
            "lowshot/lowshot_0_0.01_3.csv\t29.67\n",
            "lowshot/lowshot_0_0.01_4.csv\t29.47\n",
            "lowshot/lowshot_0_0.01_5.csv\t29.09\n",
            "lowshot/lowshot_0_0.01_6.csv\t29.77\n",
            "lowshot/lowshot_0_0.01_7.csv\t25.88\n",
            "lowshot/lowshot_0_0.01_8.csv\t27.92\n",
            "lowshot/lowshot_0_0.01_9.csv\t28.99\n",
            "lowshot/lowshot_1_0.02_0.csv\t25.29\n",
            "lowshot/lowshot_1_0.02_1.csv\t25.00\n",
            "lowshot/lowshot_1_0.02_2.csv\t24.03\n",
            "lowshot/lowshot_1_0.02_3.csv\t21.50\n",
            "lowshot/lowshot_1_0.02_4.csv\t23.54\n",
            "lowshot/lowshot_1_0.02_5.csv\t23.93\n",
            "lowshot/lowshot_1_0.02_6.csv\t27.14\n",
            "lowshot/lowshot_1_0.02_7.csv\t23.54\n",
            "lowshot/lowshot_1_0.02_8.csv\t24.61\n",
            "lowshot/lowshot_1_0.02_9.csv\t23.44\n",
            "lowshot/lowshot_2_0.05_0.csv\t21.30\n",
            "lowshot/lowshot_2_0.05_1.csv\t21.60\n",
            "lowshot/lowshot_2_0.05_2.csv\t19.65\n",
            "lowshot/lowshot_2_0.05_3.csv\t20.82\n",
            "lowshot/lowshot_2_0.05_4.csv\t20.04\n",
            "lowshot/lowshot_2_0.05_5.csv\t19.46\n",
            "lowshot/lowshot_2_0.05_6.csv\t21.60\n",
            "lowshot/lowshot_2_0.05_7.csv\t21.11\n",
            "lowshot/lowshot_2_0.05_8.csv\t21.01\n",
            "lowshot/lowshot_2_0.05_9.csv\t23.35\n",
            "lowshot/lowshot_3_0.075_0.csv\t18.29\n",
            "lowshot/lowshot_3_0.075_1.csv\t19.16\n",
            "lowshot/lowshot_3_0.075_2.csv\t17.90\n",
            "lowshot/lowshot_3_0.075_3.csv\t19.07\n",
            "lowshot/lowshot_3_0.075_4.csv\t19.84\n",
            "lowshot/lowshot_3_0.075_5.csv\t19.36\n",
            "lowshot/lowshot_3_0.075_6.csv\t19.94\n",
            "lowshot/lowshot_3_0.075_7.csv\t18.58\n",
            "lowshot/lowshot_3_0.075_8.csv\t20.04\n",
            "lowshot/lowshot_3_0.075_9.csv\t20.04\n",
            "lowshot/lowshot_4_0.1_0.csv\t19.07\n",
            "lowshot/lowshot_4_0.1_1.csv\t17.80\n",
            "lowshot/lowshot_4_0.1_2.csv\t18.77\n",
            "lowshot/lowshot_4_0.1_3.csv\t19.46\n",
            "lowshot/lowshot_4_0.1_4.csv\t17.70\n",
            "lowshot/lowshot_4_0.1_5.csv\t19.65\n",
            "lowshot/lowshot_4_0.1_6.csv\t18.77\n",
            "lowshot/lowshot_4_0.1_7.csv\t19.26\n",
            "lowshot/lowshot_4_0.1_8.csv\t18.39\n",
            "lowshot/lowshot_4_0.1_9.csv\t18.39\n",
            "lowshot/lowshot_5_0.2_0.csv\t15.66\n",
            "lowshot/lowshot_5_0.2_1.csv\t16.34\n",
            "lowshot/lowshot_5_0.2_2.csv\t16.15\n",
            "lowshot/lowshot_5_0.2_3.csv\t15.37\n",
            "lowshot/lowshot_5_0.2_4.csv\t15.27\n",
            "lowshot/lowshot_5_0.2_5.csv\t16.54\n",
            "lowshot/lowshot_5_0.2_6.csv\t17.22\n",
            "lowshot/lowshot_5_0.2_7.csv\t16.25\n",
            "lowshot/lowshot_5_0.2_8.csv\t18.00\n",
            "lowshot/lowshot_5_0.2_9.csv\t17.41\n",
            "lowshot/lowshot_6_0.5_0.csv\t14.11\n",
            "lowshot/lowshot_6_0.5_1.csv\t14.11\n",
            "lowshot/lowshot_6_0.5_2.csv\t13.52\n",
            "lowshot/lowshot_6_0.5_3.csv\t14.20\n",
            "lowshot/lowshot_6_0.5_4.csv\t13.72\n",
            "lowshot/lowshot_6_0.5_5.csv\t13.81\n",
            "lowshot/lowshot_6_0.5_6.csv\t13.04\n",
            "lowshot/lowshot_6_0.5_7.csv\t13.52\n",
            "lowshot/lowshot_6_0.5_8.csv\t14.11\n",
            "lowshot/lowshot_6_0.5_9.csv\t13.91\n",
            "lowshot/lowshot_7_0.75_0.csv\t12.65\n",
            "lowshot/lowshot_7_0.75_1.csv\t13.33\n",
            "lowshot/lowshot_7_0.75_2.csv\t12.65\n",
            "lowshot/lowshot_7_0.75_3.csv\t12.65\n",
            "lowshot/lowshot_7_0.75_4.csv\t12.94\n",
            "lowshot/lowshot_7_0.75_5.csv\t12.84\n",
            "lowshot/lowshot_7_0.75_6.csv\t12.94\n",
            "lowshot/lowshot_7_0.75_7.csv\t12.55\n",
            "lowshot/lowshot_7_0.75_8.csv\t13.13\n",
            "lowshot/lowshot_7_0.75_9.csv\t12.26\n",
            "lowshot/lowshot_8_1.0_0.csv\t11.38\n",
            "lowshot/lowshot_8_1.0_1.csv\t12.55\n",
            "lowshot/lowshot_8_1.0_2.csv\t13.04\n",
            "lowshot/lowshot_8_1.0_3.csv\t12.26\n",
            "lowshot/lowshot_8_1.0_4.csv\t11.96\n",
            "lowshot/lowshot_8_1.0_5.csv\t11.96\n",
            "lowshot/lowshot_8_1.0_6.csv\t12.16\n",
            "lowshot/lowshot_8_1.0_7.csv\t11.77\n",
            "lowshot/lowshot_8_1.0_8.csv\t12.16\n",
            "lowshot/lowshot_8_1.0_9.csv\t12.16\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}